{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwJ3u5bcGJ6e"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSn0uRJUwQa6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kHrbUG7ECb2z"
   },
   "outputs": [],
   "source": [
    "def my_normalizing(normalText):\n",
    "    nm_dic = {\n",
    "        'ي' : 'ی',\n",
    "        'ي' : 'ی',\n",
    "        'ئ' : 'ی',\n",
    "        'ي' : 'ی',\n",
    "        'ﯤ' : 'ی',\n",
    "        'ة' : 'ه',\n",
    "        'ﻙ' : 'ک',\n",
    "        'ﻚ' : 'ک',\n",
    "        'ؤ' : 'و',\n",
    "        'أ' : 'ا',\n",
    "        'إ' : 'ا',\n",
    "        'ٱ' : 'ا',    \n",
    "        'ء' : '',\n",
    "        'آ' : 'ا',\n",
    "        'ِ' : '',\n",
    "        'ُ' : '',\n",
    "        'َ' : '',\n",
    "        'ٍ' : '',\n",
    "        'ٌ' : '',\n",
    "        'ً' : '',\n",
    "        'ّ' : '',\n",
    "        'ْ' : '',\n",
    "        'ـ' : '',\n",
    "        '{' : ' { ',\n",
    "        '}' : ' } ',\n",
    "        '[' : ' [ ',\n",
    "        ']' : ' ] ',\n",
    "        '(' : ' ( ',\n",
    "        ')' : ' ) ',\n",
    "        ',' : '،',\n",
    "        '’' : ' ’ ',\n",
    "        ';' : '؛',\n",
    "        '«' : ' « ',\n",
    "        '»' : ' » ',\n",
    "        '\\u200a' : ' ',\n",
    "        '\\u200b' : ' ',\n",
    "        '\\u200c' : ' ',\n",
    "        '\\u200d' : ' ',\n",
    "        '\\u200e' : ' ',\n",
    "        '\\u200f' : ' ',\n",
    "        '‌' : ' ',\n",
    "        ':' : ' : ',\n",
    "        '!' : ' ! ',\n",
    "        '?' : ' ? ',\n",
    "        '؟' : ' ؟ ',\n",
    "        '،' : ' ، ',\n",
    "        '؛' : ' ؛ ',\n",
    "        '/' : ' / ',\n",
    "        '\\\\' : ' \\\\ ',\n",
    "        '-' : ' ',\n",
    "        '\"' : ' \" ',\n",
    "        \"'\" : \" ' \",\n",
    "        '“' : ' ',\n",
    "        '”' : ' ',\n",
    "        '‘' : ' ',\n",
    "        '..' : '.',\n",
    "        '.' : ' . ',\n",
    "        '\\n' : ' ',\n",
    "        '\\r' : ' ',\n",
    "        '\\t' : ' ',\n",
    "        '\\f' : ' ',\n",
    "        '\\v' : ' ',\n",
    "        '=' : ' = ',\n",
    "        '+' : ' + ',\n",
    "        '-' : ' - ',\n",
    "        '–' : ' – ',\n",
    "        '÷' : ' ÷ ',\n",
    "        '×' : ' × ',\n",
    "        'ـ' : ' ـ ',\n",
    "        '_' : ' _ ',\n",
    "        '!' : ' ! ',\n",
    "        '@' : ' @ ',\n",
    "        '#' : ' # ',\n",
    "        '$' : ' $ ',\n",
    "        '%' : ' % ',\n",
    "        '^' : ' ^ ',\n",
    "        '&' : ' & ',\n",
    "        '*' : ' * ',\n",
    "        '۰' : '0',\n",
    "        '۱' : '1',\n",
    "        '۲' : '2',\n",
    "        '۳' : '3',\n",
    "        '۴' : '4',\n",
    "        '۵' : '5',\n",
    "        '۶' : '6',\n",
    "        '۷' : '7',\n",
    "        '۸' : '8',\n",
    "        '۹' : '9'\n",
    "    }\n",
    "\n",
    "    for key in tqdm(nm_dic):\n",
    "        normalText = normalText.replace(key, nm_dic[key])\n",
    "\n",
    "        normalText = re.sub(r'([a-zA-Z]*)(\\d+)([a-zA-Z]*)', r'\\1 \\2 \\3', normalText)\n",
    "        normalText = re.sub(r'[a-zA-Z]+', r' \\g<0> ', normalText)\n",
    "        normalText = re.sub(r'([\\s])([ی])([\\W\\d\\s]+)', r' \\3', normalText)\n",
    "\n",
    "    \n",
    "    return normalText\n",
    "\n",
    "\n",
    "def my_tiny_normalizing(normalText):\n",
    "    nm_dic1 = {\n",
    "        'ي' : 'ی',\n",
    "        'ي' : 'ی',\n",
    "        'ئ' : 'ی',\n",
    "        'ي' : 'ی',\n",
    "        'ﯤ' : 'ی',\n",
    "        'ة' : 'ه',\n",
    "        'ﻙ' : 'ک',\n",
    "        'ﻚ' : 'ک',\n",
    "        'ؤ' : 'و',\n",
    "        'أ' : 'ا',\n",
    "        'إ' : 'ا',\n",
    "        'ٱ' : 'ا',    \n",
    "        'ء' : '',\n",
    "        'آ' : 'ا',\n",
    "        'ِ' : '',\n",
    "        '' : '',\n",
    "        'َ' : '',\n",
    "        'ٍ' : '',\n",
    "        'ٌ' : '',\n",
    "        'ً' : '',\n",
    "        'ّ' : '',\n",
    "        'ـ' : ' ',\n",
    "        '{' : ' ',\n",
    "        '}' : ' ',\n",
    "        '[' : ' ',\n",
    "        ']' : ' ',\n",
    "        '(' : ' ',\n",
    "        ')' : ' ',\n",
    "        ',' : ' ',\n",
    "        ' ' : '؛',\n",
    "        '«' : ' ',\n",
    "        '»' : ' ',\n",
    "        '\\u200a' : ' ',\n",
    "        '\\u200b' : ' ',\n",
    "        '\\u200c' : ' ',\n",
    "        '\\u200d' : ' ',\n",
    "        '\\u200e' : ' ',\n",
    "        '\\u200f' : ' ',\n",
    "        '‌' : ' ',\n",
    "        ':' : ' ',\n",
    "        '!' : ' ',\n",
    "        '?' : ' ',\n",
    "        '؟' : ' ',\n",
    "        '،' : ' ',\n",
    "        '’' : ' ',\n",
    "        '؛' : ' ',\n",
    "        ';' : ' ',\n",
    "        '/' : ' ',\n",
    "        '\\\\' : ' ',\n",
    "        '\"' : ' ',\n",
    "        '“' : ' ',\n",
    "        '”' : ' ',\n",
    "        \"'\" : ' ',\n",
    "        '..' : ' ',\n",
    "        '.' : ' ',\n",
    "        '\\n' : ' ',\n",
    "        '\\r' : ' ',\n",
    "        '\\t' : ' ',\n",
    "        '\\f' : ' ',\n",
    "        '\\v' : ' ',\n",
    "        '!' : ' ',\n",
    "        '@' : ' ',\n",
    "        '#' : ' ',\n",
    "        '$' : ' ',\n",
    "        '%' : ' ',\n",
    "        '^' : ' ',\n",
    "        '&' : ' ',\n",
    "        '*' : ' ',\n",
    "        '=' : ' ',\n",
    "        '+' : ' ',\n",
    "        '-' : ' ',\n",
    "        '–' : ' ',\n",
    "        '÷' : ' ',\n",
    "        '×' : ' ',\n",
    "        '_' : ' ',\n",
    "        'ـ' : ' ',\n",
    "        '۰' : '0',\n",
    "        '۱' : '1',\n",
    "        '۲' : '2',\n",
    "        '۳' : '3',\n",
    "        '۴' : '4',\n",
    "        '۵' : '5',\n",
    "        '۶' : '6',\n",
    "        '۷' : '7',\n",
    "        '۸' : '8',\n",
    "        '۹' : '9'\n",
    "    }\n",
    "\n",
    "    for key in tqdm(nm_dic1):\n",
    "        normalText = normalText.replace(key, nm_dic1[key])\n",
    "\n",
    "        normalText = re.sub(r'([a-zA-Z]*)(\\d+)([a-zA-Z]*)', r'\\1 \\2 \\3', normalText)\n",
    "        normalText = re.sub(r'[a-zA-Z]+', r' \\g<0> ', normalText)\n",
    "        normalText = re.sub(r'([\\s])([ی])([\\W\\d\\s]+)', r' \\3', normalText)\n",
    "\n",
    "    return normalText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Ct7JEe-CqJ8"
   },
   "outputs": [],
   "source": [
    "def remove_proNone(text):\n",
    "    proNoneList=[\n",
    "        \"در\",\n",
    "        \"بی\",\n",
    "        \"را\",\n",
    "        \"به\",\n",
    "        \"از\",\n",
    "        \"که\",\n",
    "        \"این\",\n",
    "        \"ایا\",\n",
    "        \"با\",\n",
    "        \"تا\",\n",
    "        \"می\",\n",
    "        \"نیز\",\n",
    "        \"یا\",\n",
    "        \"ما\",\n",
    "        \"باید\",\n",
    "        \"اند\",\n",
    "        \"هم\",\n",
    "        \"همچنین\",\n",
    "        \"برای\",\n",
    "        \"ها\",\n",
    "        \"ان\",\n",
    "        \"وی\",\n",
    "        \"یک\",\n",
    "        \"خود\",\n",
    "        \"بر\",\n",
    "        \"دو\",\n",
    "        \"انها\",\n",
    "        \"اما\",\n",
    "        \"دیگر\",\n",
    "        \"اگر\",\n",
    "        \"گر\",\n",
    "        \"فی\",\n",
    "        \"را\",\n",
    "        \"های\",\n",
    "        \"هایی\",\n",
    "        \"و\",\n",
    "        \"نمی\",\n",
    "        \"هر\",\n",
    "        \"ای\",\n",
    "        \"ات\",\n",
    "         \"ام\",\n",
    "         \"اش\",\n",
    "        \"تان\",\n",
    "        \"مان\",\n",
    "        \"شان\",\n",
    "        \"زیر\",\n",
    "        \"البته\",\n",
    "        \"بدین\",\n",
    "        \"چون\",\n",
    "        \"زیرا\",\n",
    "        \"فراوان\",\n",
    "        \"اند\",\n",
    "        \"اییم\",\n",
    "        \"اید\",\n",
    "        \"براساس\",\n",
    "        \"قبلا\",\n",
    "        \"یک\",\n",
    "        \"ان\",\n",
    "        \"بسیار\",\n",
    "        \"ما\",\n",
    "        \"شما\",\n",
    "        \"اینکه\",\n",
    "        \"(س)\",\n",
    "        \"(ص)\",\n",
    "        \"(ع)\",\n",
    "        \"(عج)\",\n",
    "        \"(ره)\",\n",
    "        \"(رضی)\"\n",
    "        ]\n",
    " \n",
    "        \n",
    "    for key in tqdm(proNoneList):\n",
    "        reText = r'\\b'+key+r'\\b'\n",
    "        text = re.sub(reText, r'', text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "    \n",
    "def remove_regularWords(text):\n",
    "    regularWordList=[\n",
    "        \"است\",\n",
    "        \"امدم\",\n",
    "        \"امد\",\n",
    "        \"امدن\",\n",
    "        \"امدند\",\n",
    "        \"امده\",\n",
    "        \"امدی\",\n",
    "        \"امدید\",\n",
    "        \"امدیم\",\n",
    "        \"اورد\",\n",
    "        \"اوردم\",\n",
    "        \"اوردن\",\n",
    "        \"اوردند\",\n",
    "        \"اورده\",\n",
    "        \"اوردی\",\n",
    "        \"اوردید\",\n",
    "        \"اوردیم\",\n",
    "        \"اورم\",\n",
    "        \"اورند\",\n",
    "        \"اوری\",\n",
    "        \"اورید\",\n",
    "        \"اوریم\",\n",
    "        \"اید\",\n",
    "        \"ایم\",\n",
    "        \"ايند\",\n",
    "        \"ایی\",\n",
    "        \"ایید\",\n",
    "        \"اییم\",\n",
    "        \"باش\",\n",
    "        \"باشد\",\n",
    "        \"باشند\",\n",
    "        \"باشی\",\n",
    "        \"باشم\",\n",
    "        \"باشید\",\n",
    "        \"باشیم\",\n",
    "        \"باید\",\n",
    "        \"بتوان\",\n",
    "        \"بتواند\",\n",
    "        \"بتوانم\",\n",
    "        \"بتوانی\",\n",
    "        \"بتوانیم\",\n",
    "        \"بتوانید\",\n",
    "        \"بتوانند\",\n",
    "        \"بخواه\",\n",
    "        \"بخواهم\",\n",
    "        \"بخواهد\",\n",
    "        \"بخواهند\",\n",
    "        \"بخواهی\",\n",
    "        \"بخواهید\",\n",
    "        \"بخواهیم\",\n",
    "        \"بکن\",\n",
    "        \"بکند\",\n",
    "        \"بکنم\",\n",
    "        \"بکنند\",\n",
    "        \"بکنی\",\n",
    "        \"بکنید\",\n",
    "        \"بکنیم\",\n",
    "        \"بگو\",\n",
    "        \"بگوید\",\n",
    "        \"بگویم\",\n",
    "        \"بگویند\",\n",
    "        \"بگویی\",\n",
    "        \"بگویید\",\n",
    "        \"بگوییم\",\n",
    "        \"بگیر\",\n",
    "        \"بگیرد\",\n",
    "        \"بگیرم\",\n",
    "        \"بگیرند\",\n",
    "        \"بگیری\",\n",
    "        \"بگیرید\",\n",
    "        \"بگیریم\",\n",
    "        \"بود\",\n",
    "        \"بودم\",\n",
    "        \"بودن\",\n",
    "        \"بودند\",\n",
    "        \"بوده\",\n",
    "        \"بودی\",\n",
    "        \"بودید\",\n",
    "        \"بودیم\",\n",
    "        \"بیا\",\n",
    "        \"بیاب\",\n",
    "        \"بیابد\",\n",
    "        \"بیابم\",\n",
    "        \"بیابند\",\n",
    "        \"بیابی\",\n",
    "        \"بیابید\",\n",
    "        \"بیابیم\",\n",
    "        \"بیاور\",\n",
    "        \"بیاورد\",\n",
    "        \"بیاورم\",\n",
    "        \"بیاورند\",\n",
    "        \"بیاوری\",\n",
    "        \"بیاورید\",\n",
    "        \"بیاوریم\",\n",
    "        \"بیاید\",\n",
    "        \"بیایم\",\n",
    "        \"بیایند\",\n",
    "        \"بیابی\",\n",
    "        \"بیایید\",\n",
    "        \"بیاییم\",\n",
    "        \"تواند\",\n",
    "        \"توانست\",\n",
    "        \"توانستم\",\n",
    "        \"توانستن\",\n",
    "        \"توانستید\",\n",
    "        \"توانسته\",\n",
    "        \"توانستی\",\n",
    "        \"توانستند\",\n",
    "        \"توانستیم\",\n",
    "        \"توانم\",\n",
    "        \"توانند\",\n",
    "        \"توانی\",\n",
    "        \"توانید\",\n",
    "        \"توانیم\",\n",
    "        \"خواست\",\n",
    "        \"خواستم\",\n",
    "        \"خواستن\",\n",
    "        \"خواستند\",\n",
    "        \"خواسته\",\n",
    "        \"خواستی\",\n",
    "        \"خواستید\",\n",
    "        \"خواستیم\",\n",
    "        \"خواهد\",\n",
    "        \"خواهم\",\n",
    "        \"خواهند\",\n",
    "        \"خواهی\",\n",
    "        \"خواهید\",\n",
    "        \"خواهیم\",\n",
    "        \"داد\",\n",
    "        \"دار\",\n",
    "        \"دارد\",\n",
    "        \"دارم\",\n",
    "        \"دارند\",\n",
    "        \"داری\",\n",
    "        \"دارید\",\n",
    "        \"داریم\",\n",
    "        \"داشت\",\n",
    "        \"داشتم\",\n",
    "        \"داشتن\",\n",
    "        \"داشتند\",\n",
    "        \"داشته\",\n",
    "        \"داشتی\",\n",
    "        \"داشتید\",\n",
    "        \"داشتیم\",\n",
    "        \"شد\",\n",
    "        \"شده\",\n",
    "        \"شود\",\n",
    "        \"کرد\",\n",
    "        \"کردم\",\n",
    "        \"کردن\",\n",
    "        \"کردند\",\n",
    "        \"کرده\",\n",
    "        \"کردی\",\n",
    "        \"کردید\",\n",
    "        \"کردیم\",\n",
    "        \"کن\",\n",
    "        \"کند\",\n",
    "        \"کنم\",\n",
    "        \"کنند\",\n",
    "        \"کنی\",\n",
    "        \"کنید\",\n",
    "        \"کنیم\",\n",
    "        \"گرفت\",\n",
    "        \"گرفتم\",\n",
    "        \"گرفتن\",\n",
    "        \"گرفتند\",\n",
    "        \"گرفته\",\n",
    "        \"گرفتی\",\n",
    "        \"گرفتید\",\n",
    "        \"گرفتیم\",\n",
    "        \"گفت\",\n",
    "        \"گفتم\",\n",
    "        \"گفتن\",\n",
    "        \"گفتند\",\n",
    "        \"گفته\",\n",
    "        \"گفتی\",\n",
    "        \"گفتید\",\n",
    "        \"گفتیم\",\n",
    "        \"گوید\",\n",
    "        \"گویم\",\n",
    "        \"گویند\",\n",
    "        \"گویی\",\n",
    "        \"گویید\",\n",
    "        \"گوییم\",\n",
    "        \"گیرد\",\n",
    "        \"گیرم\",\n",
    "        \"گیرند\",\n",
    "        \"گیری\",\n",
    "        \"گیرید\",\n",
    "        \"گیریم\",\n",
    "        \"هست\",\n",
    "        \"هستم\",\n",
    "        \"هستند\",\n",
    "        \"هستی\",\n",
    "        \"هستید\",\n",
    "        \"هستیم\",\n",
    "        \"یابد\",\n",
    "        \"یابم\",\n",
    "        \"یابند\",\n",
    "        \"یابی\",\n",
    "        \"یابید\",\n",
    "        \"یابیم\",\n",
    "        \"یافت\",\n",
    "        \"یافتم\",\n",
    "        \"یافتن\",\n",
    "        \"یافتند\",\n",
    "        \"یافته\",\n",
    "        \"یافتی\",\n",
    "        \"یافتید\",\n",
    "        \"یافتیم\",\n",
    "        \"برسد\",\n",
    "        \"برسم\",\n",
    "        \"برسی\",\n",
    "        \"برسیم\",\n",
    "        \"برسید\",\n",
    "        \"برسند\"\n",
    "    ]\n",
    "    \n",
    "    for key in tqdm(regularWordList):\n",
    "        reText = r'\\b'+key+r'\\b'\n",
    "        text = re.sub(reText, r'', text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7y2NCYxxCxoi"
   },
   "outputs": [],
   "source": [
    "def my_TF(mainDic, axDic):\n",
    "    tmpDic = []\n",
    "    for i in mainDic:    \n",
    "        if i in axDic:\n",
    "            tmpDic.append(axDic.get(i))\n",
    "        else:\n",
    "            tmpDic.append(0)\n",
    "            \n",
    "    return tmpDic\n",
    "\n",
    "\n",
    "def my_TF_IDF(mainDic, axDic):\n",
    "    tmpDic = []\n",
    "    for i in range(len(mainDic)):\n",
    "        tmpDic.append(mainDic[i]/axDic[i])\n",
    "    \n",
    "    return tmpDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TTNMxIfqC4dk"
   },
   "outputs": [],
   "source": [
    "def my_token_conter(tokens):\n",
    "    dic = {}\n",
    "    for wordTkn in tqdm(tokens):\n",
    "        if wordTkn in dic:\n",
    "            tknCount = dic.get(wordTkn)\n",
    "            dic.update({wordTkn:tknCount+1})\n",
    "        else:\n",
    "            dic[wordTkn] = 1\n",
    "    return dic\n",
    "\n",
    "def my_DF_conter(dic, tokens):\n",
    "    for wordTkn in tqdm(tokens):\n",
    "        #tknTokensCount = tokens.get(wordTkn)\n",
    "        if wordTkn in dic:\n",
    "            tknDicCount = dic.get(wordTkn)\n",
    "            dic.update({wordTkn:tknDicCount+1})\n",
    "        else:\n",
    "            dic[wordTkn] = 1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYQQN67QC7XX"
   },
   "outputs": [],
   "source": [
    "def my_word_tokenize(wordTokenize):\n",
    "    wt = wordTokenize.split(\" \")\n",
    "    # Split with lots of the signs, but it is not working good as I think.\n",
    "    # It is need to complete or debug.\n",
    "    #\n",
    "    # wt = re.split(r'(\\W+)', wordTokenize)\n",
    "    return wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLtBrDWdC98R"
   },
   "outputs": [],
   "source": [
    "def cos_sim_vectors(vec1, vec2):    \n",
    "    return dot(vec1, vec2)/(norm(vec1)*norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NeWmaPwNDIrm"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def macDirExistCreate(dirName):\n",
    "    if not os.path.isdir(dirName):\n",
    "        os.makedirs(dirName)\n",
    "        print(dirName + ' directry has been created!')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h_YlxDXZDQhK"
   },
   "outputs": [],
   "source": [
    "def my_file_reader(myFile, fileEncode):\n",
    "    fileContent = open(myFile, encoding = fileEncode)\n",
    "    return fileContent.read()\n",
    "\n",
    "\n",
    "def my_floatFile_reader(myFile):\n",
    "    fileContent = open(myFile, 'r')\n",
    "    x = [float(line.strip()) for line in fileContent if line]    \n",
    "    return x\n",
    "\n",
    "\n",
    "def my_Json_reader(myFile, fileEncode, keyName):\n",
    "    with open(myFile, 'r', encoding = fileEncode) as fp:\n",
    "        obj = json.load(fp, strict=False)\n",
    "    return obj[keyName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121931
    },
    "colab_type": "code",
    "id": "0ltjwKkjDd-p",
    "outputId": "8bf036d1-42e7-4136-de21-f525df54e714"
   },
   "outputs": [],
   "source": [
    "startTime = datetime.datetime.now()\n",
    "\n",
    "#macDirExistCreate('tf')\n",
    "#macDirExistCreate('tf_idf')\n",
    "#macDirExistCreate('tokenized')\n",
    "#myTokenizedPath = \"tokenized/\"\n",
    "#myTF_IDF_path = \"tf_idf/\"\n",
    "#myTF_path = \"tf/\"\n",
    "myCorpusPath = \"irna_corpus\"    \n",
    "\n",
    "txtFileList = sorted(glob.glob(f'{myCorpusPath}/*.txt'), key=os.path.basename)\n",
    "corpusLen = len(txtFileList)\n",
    "corpusLen = 5000\n",
    "\n",
    "corpusContentJson = []\n",
    "corpusTokens = {}\n",
    "for corpusNum in tqdm(range(corpusLen)):\n",
    "    fileContent = my_Json_reader(txtFileList[corpusNum], \"UTF-8\", 'text')\n",
    "    fileContent = my_tiny_normalizing(fileContent)\n",
    "    fileContent = remove_proNone(fileContent)\n",
    "    fileContent = remove_regularWords(fileContent)\n",
    "    fileContent = my_word_tokenize(fileContent)\n",
    "    fileContent = my_token_conter(fileContent)\n",
    "    corpusContentJson.append(fileContent)\n",
    "    \n",
    "    corpusTokens = my_DF_conter(corpusTokens, fileContent)\n",
    "\n",
    "\n",
    "corpusTfJson = []\n",
    "for rowKeys in tqdm(range(corpusLen)):\n",
    "    corpusTfJson.append(my_TF(corpusTokens, dict(corpusContentJson[rowKeys])))\n",
    "\n",
    "corpusTfIdfJson = []\n",
    "for rowKeys in tqdm(range(corpusLen)):    \n",
    "    corpusTfIdfJson.append(my_TF_IDF(corpusTfJson[rowKeys], list(corpusTokens.values())))\n",
    "    \n",
    "corpusSimilarity = [[] for i in range(corpusLen)]\n",
    "for rowKeys in tqdm(range(corpusLen)):  \n",
    "    similarityTf    = cos_sim_vectors(corpusTfJson[0], corpusTfJson[rowKeys])\n",
    "    similarityTfIdf = cos_sim_vectors(corpusTfIdfJson[0], corpusTfIdfJson[rowKeys])\n",
    "    corpusSimilarity[rowKeys].append(similarityTf)\n",
    "    corpusSimilarity[rowKeys].append(similarityTfIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EAn8W922afAI"
   },
   "outputs": [],
   "source": [
    "endTime = datetime.datetime.now()\n",
    "totalTime = endTime - startTime\n",
    "\n",
    "print(\" --- FINISHED in \",totalTime.total_seconds(),\" seconds --- \")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "frNLTK.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
